{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets transformers librosa soundfile jiwer evaluate --quiet\n",
    "# %pip install pytorch_lightning torch bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Datamodule, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import librosa\n",
    "import pytorch_lightning as pl\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from datasets import load_dataset\n",
    "from easydict import EasyDict as edict\n",
    "from evaluate import load as load_metric\n",
    "from IPython.display import Audio\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from transformers.models.whisper.modeling_whisper import WhisperEncoder\n",
    "\n",
    "\n",
    "class VietBud500DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        processor_name: str = \"vinai/PhoWhisper-tiny\",\n",
    "        num_workers: int = 2,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.processor = AutoProcessor.from_pretrained(processor_name)\n",
    "\n",
    "        print(\"Download just 3 shards / 105 shards of the origin training data\")\n",
    "        self.train_url = [\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00000-of-00105-be5f872f8be772f5.parquet\",\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00097-of-00105-4160c0470220c086.parquet\",\n",
    "            \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/train-00086-of-00105-131a0bbf617d895c.parquet\",\n",
    "        ]\n",
    "        self.test_url = \"https://huggingface.co/datasets/linhtran92/viet_bud500/resolve/main/data/test-00000-of-00002-531c1d81edb57297.parquet\"\n",
    "        self.data_files = {\"train\": self.train_url, \"test\": self.test_url}\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.dataset = load_dataset(\n",
    "            \"parquet\",\n",
    "            data_files=self.data_files,\n",
    "        )\n",
    "        self.sampling_rate = self.dataset[\"train\"].features[\"audio\"].sampling_rate\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        test_dataset = self.dataset[\"test\"]\n",
    "\n",
    "        train_dataset = self.dataset[\"train\"].shuffle(seed=42)\n",
    "        train_val_split = train_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "        self.train_dataset = train_val_split[\"train\"]\n",
    "        self.val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "        print(\n",
    "            \"Just select 1000 examples from a shard of the origin test data serving as the test split!\"\n",
    "        )\n",
    "        self.test_dataset = test_dataset.select(range(1000))\n",
    "\n",
    "        print(\"Number of training examples:\", len(self.train_dataset))\n",
    "        print(\"Number of validation examples:\", len(self.val_dataset))\n",
    "        print(\"Number of test examples:\", len(self.test_dataset))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Extract audio and transcription from the batch\n",
    "        audios = [item[\"audio\"][\"array\"] for item in batch]\n",
    "        transcriptions = [item[\"transcription\"] for item in batch]\n",
    "\n",
    "        # Process audio and transcription using the processor\n",
    "        inputs = self.processor(\n",
    "            audios,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Tokenize transcriptions\n",
    "        labels = self.processor(\n",
    "            text=transcriptions,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "        ).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": inputs.input_features,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoWhisperLightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/PhoWhisper-tiny\",\n",
    "        learning_rate: float = 5e-5,\n",
    "        warmup_steps: int = 1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # Save hyperparameters for logging\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        # self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        temp_model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.encoder = WhisperEncoder(config=self.config)\n",
    "        self.encoder.load_state_dict(temp_model.model.encoder.state_dict(), strict=True)\n",
    "        del temp_model\n",
    "\n",
    "        self.ctc_head = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, self.config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.Linear(self.config.hidden_size, self.processor.tokenizer.vocab_size),\n",
    "        )\n",
    "\n",
    "        self.ctc_loss = torch.nn.CTCLoss(\n",
    "            blank=self.processor.tokenizer.pad_token_id, zero_infinity=True\n",
    "        )\n",
    "\n",
    "        # Hyperparameters for AdamW optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def forward(self, input_features, labels=None):\n",
    "        encoder_outputs = self.encoder(input_features)  # (batch, time, hidden)\n",
    "        logits = self.ctc_head(encoder_outputs.last_hidden_state)  # (batch, time, vocab)\n",
    "        logits = logits.transpose(0, 1)  # (time, batch, vocab)\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "        input_lengths = torch.full(\n",
    "            size=(log_probs.size(1),),\n",
    "            fill_value=log_probs.size(0),\n",
    "            dtype=torch.int32,\n",
    "        )\n",
    "        if labels is not None:\n",
    "\n",
    "            # replace first bos token by pad token (blank token)\n",
    "            labels[labels == self.processor.tokenizer.bos_token_id] = (\n",
    "                self.processor.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            label_mask = labels != self.processor.tokenizer.pad_token_id\n",
    "            labels = labels[label_mask].to(torch.int32)\n",
    "            label_lengths = label_mask.sum(dim=1)\n",
    "            assert label_lengths.sum() == labels.size(\n",
    "                0\n",
    "            )  # \"Sum of label_lengths must equal number of labels.\"\n",
    "\n",
    "            loss = self.ctc_loss(log_probs, labels, input_lengths, label_lengths)\n",
    "            self.log(\n",
    "                \"train_loss\",\n",
    "                loss,\n",
    "                on_step=True,\n",
    "                on_epoch=True,\n",
    "                prog_bar=True,\n",
    "                logger=True,\n",
    "            )\n",
    "            return edict(\n",
    "                {\n",
    "                    \"loss\": loss,\n",
    "                    \"logits\": logits if labels is not None else None,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return edict({\"logits\": logits})\n",
    "\n",
    "    def seq2seq_forward(self, input_features, labels=None):\n",
    "        # Seq2Seq forward pass\n",
    "        # Current not used\n",
    "        return self.model(input_features=input_features, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_features = batch[\"input_features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=0.1,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-6,\n",
    "        )\n",
    "        # optimizer = Adam8bit(self.parameters(), lr=self.learning_rate, eps=1e-8)\n",
    "        train_dataloader = self.trainer.datamodule.train_dataloader()\n",
    "        total_steps = len(train_dataloader) * self.trainer.max_epochs\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # Save the processor along with the model checkpoint\n",
    "        checkpoint[\"processor\"] = self.processor\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        self.processor = checkpoint[\"processor\"]\n",
    "\n",
    "    def ctc_decode(self, logits, processor=None):\n",
    "        if processor is None:\n",
    "            processor = self.processor\n",
    "        # logits shape: (time, batch, vocab)\n",
    "        logits = logits.transpose(0, 1)  # (batch, time, vocab)\n",
    "        class_indices = logits.argmax(dim=2)\n",
    "        texts = []\n",
    "        for seq in class_indices:\n",
    "            # Remove blanks (pad tokens)\n",
    "            seq_no_blank = seq[seq != processor.tokenizer.pad_token_id]\n",
    "            # Collapse repeats\n",
    "            seq_collapsed = []\n",
    "            prev_token = -1\n",
    "            for token in seq_no_blank:\n",
    "                if token != prev_token:\n",
    "                    seq_collapsed.append(token.item())\n",
    "                    prev_token = token\n",
    "            # Decode to text\n",
    "            text = processor.decode(seq_collapsed, skip_special_tokens=False)\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_evaluate(pl_module, test_dataloader, device=\"cuda\"):\n",
    "    # This legacy function is uses for Seq2Seq model, currently not used\n",
    "    # Load the WER metric\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "    # Initialize lists to hold predictions and references\n",
    "    predictions = []\n",
    "    references = []\n",
    "    pl_module.to(device)\n",
    "    # Set the model to evaluation mode\n",
    "    pl_module.eval()\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dataloader):\n",
    "                # Move input features and labels to the correct device\n",
    "                input_features = batch[\"input_features\"].to(pl_module.device)\n",
    "                labels = batch[\"labels\"].to(pl_module.device)\n",
    "\n",
    "                # Generate outputs\n",
    "                outputs = pl_module.model.generate(input_features=input_features, do_sample=True)\n",
    "                # Decode generated outputs to text\n",
    "                predicted_texts = datamodule.processor.batch_decode(\n",
    "                    outputs, skip_special_tokens=True\n",
    "                )\n",
    "                # Handle labels: replace -100 with pad_token_id and decode\n",
    "                labels_cpu = labels.detach().cpu()\n",
    "                label_texts = datamodule.processor.batch_decode(\n",
    "                    labels_cpu, skip_special_tokens=True\n",
    "                )\n",
    "                # Collect predictions and references\n",
    "                predictions.extend(predicted_texts)\n",
    "                references.extend(label_texts)\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    # Return the results as a dictionary\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "def wer_ctc_evaluate(pl_module, test_dataloader, device=\"cuda\"):\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    pl_module.to(device)\n",
    "    pl_module.eval()\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_dataloader):\n",
    "                input_features = batch[\"input_features\"].to(pl_module.device)\n",
    "                labels = batch[\"labels\"].to(pl_module.device)\n",
    "                logits = pl_module(input_features=input_features, labels=None).logits\n",
    "\n",
    "                predicted_texts = pl_module.ctc_decode(logits)\n",
    "                label_texts = pl_module.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                predictions.extend(predicted_texts)\n",
    "                references.extend(label_texts)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    print(\"First 5 predictions: \", predictions[:5])\n",
    "    print(\"First 5 references: \", references[:5])\n",
    "    print(\"WER:\", wer)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "class EvalCallback(pl.Callback):\n",
    "    def __init__(self, processor):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.val_predicted_texts = []\n",
    "        self.val_reference_texts = []\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        logits = outputs.logits.detach().cpu()\n",
    "        labels = batch[\"labels\"].detach().cpu()\n",
    "        predicted_texts = self.ctc_decode(logits, self.processor)\n",
    "        reference_texts = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "        self.val_predicted_texts.extend(predicted_texts)\n",
    "        self.val_reference_texts.extend(reference_texts)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        wer_metric = load_metric(\"wer\")\n",
    "        wer = wer_metric.compute(\n",
    "            predictions=self.val_predicted_texts, references=self.val_reference_texts\n",
    "        )\n",
    "        pl_module.log(\"val_wer\", wer, prog_bar=True, logger=True)\n",
    "        print(\"WER on validate data:\", wer)\n",
    "        print(\"First 5 predictions: \", self.val_predicted_texts[:5])\n",
    "        print(\"First 5 references: \", self.val_reference_texts[:5])\n",
    "\n",
    "        # Clear the lists for the next epoch\n",
    "        self.val_predicted_texts = []\n",
    "        self.val_reference_texts = []\n",
    "\n",
    "    def ctc_decode(self, logits, processor):\n",
    "        # logits shape: (time, batch, vocab)\n",
    "        # Transpose to (batch, time, vocab)\n",
    "        logits = logits.transpose(0, 1)\n",
    "        # Get the class indices\n",
    "        class_indices = logits.argmax(dim=2)\n",
    "        # Remove blanks and collapse repeats for each sequence\n",
    "        texts = []\n",
    "        for seq in class_indices:\n",
    "            # Remove blanks (pad tokens)\n",
    "            seq_no_blank = seq[seq != processor.tokenizer.pad_token_id]\n",
    "            # Collapse repeats\n",
    "            seq_collapsed = []\n",
    "            prev_token = -1\n",
    "            for token in seq_no_blank:\n",
    "                if token != prev_token:\n",
    "                    seq_collapsed.append(token.item())\n",
    "                    prev_token = token\n",
    "            # Decode to text\n",
    "            text = processor.decode(seq_collapsed, skip_special_tokens=False)\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data module\n",
    "datamodule = VietBud500DataModule(batch_size=24, processor_name=\"vinai/PhoWhisper-tiny\")\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "\n",
    "# Initialize the Lightning module\n",
    "lightning_module = PhoWhisperLightningModule(\n",
    "    model_name=\"vinai/PhoWhisper-tiny\", learning_rate=1e-4, warmup_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_module = PhoWhisperLightningModule.load_from_checkpoint(\n",
    "    \"./lightning_logs/version_29/checkpoints/best-val_wer=0.3986.ckpt\"\n",
    ")\n",
    "print(\"Evaluate after training\", wer_ctc_evaluate(lightning_module, datamodule.test_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The evaluation result on test set is 0.41 of WER (Word Error Rate).\n",
    "- !ï¿½ is a bug in text label tokenzation during training. The first two special tokens shouldn't be included in the text label tokenization. It 's my mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Best Checkpoint to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload checkpoint ./lightning_logs/version_29/checkpoints/best-val_wer=0.3986.ckpt\n",
    "# to huggingface at repo: tuandunghcmut/PhoWhisper-tiny-CTC\n",
    "# import huggingface_hub\n",
    "# import os\n",
    "\n",
    "# checkpoint_path = \"./lightning_logs/version_29/checkpoints/best-val_wer=0.3986.ckpt\"\n",
    "# model_id = \"tuandunghcmut/PhoWhisper-tiny-CTC\"\n",
    "# model_hub = huggingface_hub.HfApi()\n",
    "# model_hub.create_repo(model_id, exist_ok=True)\n",
    "# model_hub.upload_file(\n",
    "#     path_or_fileobj=checkpoint_path,\n",
    "#     path_in_repo=\"best-val_wer=0.3986.ckpt\",\n",
    "#     repo_id=model_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download checkpoint from HuggingFace hub\n",
    "- Everyone can download the checkpoint from HuggingFace hub and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ckpt_path = \"best-val_wer=0.3986.ckpt\"\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# download the checkpoint from huggingface\n",
    "hf_hub_download(\n",
    "    repo_id=model_id,\n",
    "    filename=\"best-val_wer=0.3986.ckpt\",\n",
    "    local_dir=\"./\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_module = PhoWhisperLightningModule.load_from_checkpoint(\"best-val_wer=0.3986.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, processor, audio_path, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    audio, _ = librosa.load(audio_path, sr=16000)\n",
    "    input_features = processor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_features\n",
    "    input_features = input_features.to(model.device)\n",
    "    logits = model(input_features=input_features).logits\n",
    "    predicted_text = model.ctc_decode(logits, processor)[0]\n",
    "    predicted_text = predicted_text[2:]  # remove the first token, due to my tokenization mistake\n",
    "    return predicted_text\n",
    "\n",
    "\n",
    "def get_random_test_audio(datamodule):\n",
    "    random_index = random.randint(0, len(datamodule.test_dataset))\n",
    "    audio_obj = datamodule.test_dataset[random_index][\"audio\"]\n",
    "    transcription = datamodule.test_dataset[random_index][\"transcription\"]\n",
    "    print(\"Ground True Transcription:\", transcription)\n",
    "    audio_path = \"random_test_audio.wav\"\n",
    "    sf.write(audio_path, audio_obj[\"array\"], 16000)\n",
    "    display(Audio(audio_path))\n",
    "    return audio_obj, transcription\n",
    "\n",
    "\n",
    "def get_random_gd_pred_pairs(datamodule, model, num_examples=10):\n",
    "    random_indices = random.sample(range(len(datamodule.test_dataset)), num_examples)\n",
    "    # Print the ground truth and predicted transcriptions, also display the audio\n",
    "    for index in random_indices:\n",
    "        audio_obj = datamodule.test_dataset[index][\"audio\"]\n",
    "        transcription = datamodule.test_dataset[index][\"transcription\"]\n",
    "        audio_path = \"random_test_audio.wav\"\n",
    "        sf.write(audio_path, audio_obj[\"array\"], 16000)\n",
    "        display(Audio(audio_path))\n",
    "        predicted_text = predict(model, datamodule.processor, audio_path)\n",
    "        print(\"Ground True Transcription:\", transcription)\n",
    "        print(\"Predicted Transcription:\", predicted_text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "get_random_gd_pred_pairs(datamodule, lightning_module, num_examples=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WER result: 0.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_module = PhoWhisperLightningModule.load_from_checkpoint(\"./best-val_wer=0.3986.ckpt\")\n",
    "\n",
    "print(\"Evaluate after training\", wer_ctc_evaluate(lightning_module, datamodule.test_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Speed: 1000 examples in 20 seconds <=> each example averagely takes 0.02 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
